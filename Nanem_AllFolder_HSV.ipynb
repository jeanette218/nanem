{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nanem-AllFolder-HSV.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMs5tL+f/Vt0NRd6xGZb6B3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimimumemo/nanem/blob/main/Nanem_AllFolder_HSV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTxBKdfHf1VY",
        "outputId": "0f2c2e4c-89c7-4ca2-c0ed-a8ecabdbb71f"
      },
      "source": [
        "!pip install -U tensorflow-addons\n",
        "!pip install -q keras\n",
        "!pip install split-folders\n",
        "!pip install tensorflow-gpu"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-addons\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/4b/e893d194e626c24b3df2253066aa418f46a432fdb68250cde14bf9bb0700/tensorflow_addons-0.13.0-cp37-cp37m-manylinux2010_x86_64.whl (679kB)\n",
            "\r\u001b[K     |▌                               | 10kB 16.1MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 22.5MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 26.0MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 29.3MB/s eta 0:00:01\r\u001b[K     |██▍                             | 51kB 11.7MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 13.3MB/s eta 0:00:01\r\u001b[K     |███▍                            | 71kB 9.2MB/s eta 0:00:01\r\u001b[K     |███▉                            | 81kB 10.0MB/s eta 0:00:01\r\u001b[K     |████▍                           | 92kB 8.4MB/s eta 0:00:01\r\u001b[K     |████▉                           | 102kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 112kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 122kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 133kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 143kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 153kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 163kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 174kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 184kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 194kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 204kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 215kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 225kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 235kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 245kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████                    | 256kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 266kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 276kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 286kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 296kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 307kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 317kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 327kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 337kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 348kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 358kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 368kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 378kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 389kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 399kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 409kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 419kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 430kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 440kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 450kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 460kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 471kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 481kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 491kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 501kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 512kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 522kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 532kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 542kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 552kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 563kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 573kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 583kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 593kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 604kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 614kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 624kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 634kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 645kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 655kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 665kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 675kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 686kB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.13.0\n",
            "Collecting split-folders\n",
            "  Downloading https://files.pythonhosted.org/packages/b8/5f/3c2b2f7ea5e047c8cdc3bb00ae582c5438fcdbbedcc23b3cc1c2c7aae642/split_folders-0.4.3-py3-none-any.whl\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.4.3\n",
            "Collecting tensorflow-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/a2/5ccf0a418eb22e0a2ae9edc1e7f5456d0a4b8b49524572897564b4030a9b/tensorflow_gpu-2.5.0-cp37-cp37m-manylinux2010_x86_64.whl (454.3MB)\n",
            "\u001b[K     |████████████████████████████████| 454.3MB 36kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.19.5)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.12.1)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.12)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.6.3)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.12.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.34.1)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.36.2)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.3.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.7.4.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.1.2)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.5.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.12.4)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.4.0)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow-gpu) (1.5.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (1.30.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (1.8.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (57.0.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (0.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (2.23.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-gpu) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-gpu) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow-gpu) (4.0.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu) (2020.12.5)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-gpu) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow-gpu) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow-gpu) (3.1.0)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SQ4usBAf8vJ",
        "outputId": "989fdc60-a311-4d39-ccd8-497b17064267"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRATGEnDf_AY"
      },
      "source": [
        "import cv2 as cv2\n",
        "import itertools\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import time\n",
        "import tensorflow_addons as tfa\n",
        "import uuid\n",
        "import math\n",
        "\n",
        "from glob import glob\n",
        "from IPython.display import clear_output\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezNLjs-Jioir",
        "outputId": "7d9c5191-2988-4559-d810-8d34f864bd64"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhuvqpnMgBPO",
        "outputId": "f240941a-d765-4448-b968-049cd069aa39"
      },
      "source": [
        "module_selection = (\"mobilenet_v2_100_224\", 224)\n",
        "handle_base, pixels = module_selection\n",
        "MODULE_HANDLE =\"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\".format(handle_base)\n",
        "IMAGE_SIZE = (224, 224)\n",
        "print(\"Using {} with input size {}\".format(MODULE_HANDLE, IMAGE_SIZE))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1 with input size (224, 224)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLhaNFp8gx0a"
      },
      "source": [
        "BASE_PATH = '/content/drive/MyDrive/download_baru_4-224'\n",
        "OUTPUT_PATH = '/content/drive/MyDrive/download_baru_4_hsv'\n",
        "IMG_SHAPE = (224,224,3)\n",
        "\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
        "                                               include_top=False, \n",
        "                                               weights='imagenet')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pw9jsyk-mRv0",
        "outputId": "5156366e-908b-43d6-ce1b-4affddc30791"
      },
      "source": [
        "# To know how many picture in the datasets\n",
        "\n",
        "num_files = 0\n",
        "\n",
        "for folder in os.listdir(BASE_PATH):\n",
        "  files = os.listdir(os.path.join(BASE_PATH, folder))\n",
        "  # Calculate num files for image every folder\n",
        "  print(\"{} folder has {} pictures\".format(folder, len(files)))\n",
        "  num_files += len(files)\n",
        "print(\"\\nThere are {} pictures in total\".format(num_files))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Latosol folder has 398 pictures\n",
            "Alluvial folder has 354 pictures\n",
            "Humus folder has 335 pictures\n",
            "Clay folder has 334 pictures\n",
            "\n",
            "There are 1421 pictures in total\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC8127yUmfdw"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "# Simple progressBar to monitor progress :D\n",
        "# Credits: https://www.mikulskibartosz.name/how-to-display-a-progress-bar-in-jupyter-notebook/\n",
        "\n",
        "def updateProgressBar(progress, bar_length = 50):\n",
        "    if isinstance(progress, int):\n",
        "        progress = float(progress)\n",
        "    if not isinstance(progress, float):\n",
        "        progress = 0\n",
        "    if progress < 0:\n",
        "        progress = 0\n",
        "    if progress >= 1:\n",
        "        progress = 1\n",
        "\n",
        "    block = int(round(bar_length * progress))\n",
        "\n",
        "    # Clear current cell output \n",
        "    clear_output(wait = True)\n",
        "    # Print progress!\n",
        "    print(\"Progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvlBXiarhDX0"
      },
      "source": [
        "We are using Munsell Soil Chart \n",
        "\n",
        "Credits to \n",
        "\n",
        "https://digdays.org/wp-content/uploads/2015/10/Munsell-Soil-Color-Description-Terms1-1.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyO1m_vSg2cu"
      },
      "source": [
        "base_model.trainable = False"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDd6hCEthA1r"
      },
      "source": [
        "#Munsell name MAGHEMITE 2.5YR 3/4\n",
        "rgb_alluvial = np.uint8([[[83,11,14]]])  # 3d array just because this is what cvtColor expects...\n",
        "hsv_alluvial = cv2.cvtColor(rgb_alluvial, cv2.COLOR_RGB2HSV)[0, 0, :]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIvE1e6JiJOA"
      },
      "source": [
        "#Munsell name HUMUS 10YR 3/1\n",
        "rgb_black = np.uint8([[[42, 52, 50]]])  # 3d array just because this is what cvtColor expects...\n",
        "hsv_black = cv2.cvtColor(rgb_black, cv2.COLOR_RGB2HSV)[0, 0, :]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1Ex1CyFiMSA"
      },
      "source": [
        "#Munsell N 6/0\n",
        "rgb_clay = np.uint8([[[128,132,133]]])  # 3d array just because this is what cvtColor expects...\n",
        "hsv_clay = cv2.cvtColor(rgb_clay, cv2.COLOR_RGB2HSV)[0, 0, :]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQxs0gXyiOwg"
      },
      "source": [
        "#Munsell 10R 3/6\n",
        "rgb_red = np.uint8([[[96,15,14]]])  # 3d array just because this is what cvtColor expects...\n",
        "hsv_red = cv2.cvtColor(rgb_red, cv2.COLOR_RGB2HSV)[0, 0, :]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8U5jkhWiSJQ",
        "outputId": "60c28940-2fad-474a-f025-80183ddfcd17"
      },
      "source": [
        "# Check output location\n",
        "if os.path.isdir(OUTPUT_PATH) == False:\n",
        "    os.mkdir(OUTPUT_PATH)\n",
        "\n",
        "num_files = 0\n",
        "cnt = 0\n",
        "\n",
        "for folder in os.listdir(BASE_PATH):\n",
        "  files = os.listdir(os.path.join(BASE_PATH, folder))\n",
        "   # Calculate num files for progress\n",
        "  num_files += len(files)\n",
        "  # Check output label folder exists\n",
        "  if os.path.exists(os.path.join(OUTPUT_PATH, folder)) == False:\n",
        "    os.mkdir(os.path.join(OUTPUT_PATH, folder))\n",
        "    print('folder output not found, create new!')\n",
        "\n",
        "# Iterate all image location\n",
        "for folder in os.listdir(BASE_PATH):\n",
        "  folder_path = os.path.join(BASE_PATH, folder)\n",
        "  if folder == 'Alluvial':\n",
        "    #Munsell 2.5YR 2/3\n",
        "    lower_th = hsv_alluvial-np.array([46, 1, 8])\n",
        "    #Munsell 2.5YR 3/6\n",
        "    upper_th = hsv_alluvial+np.array([97, 20, 11])\n",
        "  \n",
        "  elif folder == 'Black':\n",
        "    #Munsell 10BG 2/1\n",
        "    lower_th = hsv_black-np.array([42, 49, 49])\n",
        "    #Munsell 5BG 5/1\n",
        "    upper_th = hsv_black+np.array([114, 126, 124])\n",
        "\n",
        "  elif folder == 'Clay':\n",
        "    #Munsell N 4/0\n",
        "    lower_th = hsv_black-np.array([60, 65, 73])\n",
        "    #Munsell N 8/0\n",
        "    upper_th = hsv_black+np.array([195, 194, 189])\n",
        "    \n",
        "\n",
        "  elif folder == 'Red':\n",
        "    #Munsell 10R 3/2\n",
        "    lower_th = hsv_red-np.array([61,25,27])\n",
        "    #Munsell 10R 4/6\n",
        "    upper_th = hsv_red+np.array([116,28,20])\n",
        "\n",
        "  # Iterate all images at current folder location\n",
        "  for image in os.listdir(folder_path):\n",
        "    image_path = os.path.join(BASE_PATH, folder, image)\n",
        "    filename, ext = os.path.splitext(image_path)\n",
        "    if os.path.isfile(image_path) == False:\n",
        "      print('{}, not found, skipping...'.format(image))\n",
        "      continue\n",
        "    \n",
        "    print('Processing image {} ...'.format(image))\n",
        "    image_read = cv2.imread(image_path)\n",
        "    image_hsv = cv2.cvtColor(image_read, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "    # Threshold the HSV image\n",
        "    mask = cv2.inRange(image_hsv, lower_th, upper_th)\n",
        "    rgb_res = cv2.bitwise_and(image_read, image_read, mask = mask)\n",
        "\n",
        "    # Save Image\n",
        "    output_location = os.path.join(OUTPUT_PATH, folder, image)\n",
        "    cv2.imwrite(output_location, rgb_res)\n",
        "    print('Done Processed image {} at {}'.format(image, output_location))\n",
        "\n",
        "    cnt += 1\n",
        "    updateProgressBar(cnt / num_files)  \n",
        "\n",
        "updateProgressBar(1)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Progress: [##################################################] 100.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUqetzfQm7Ny",
        "outputId": "982d64cb-7888-4a61-aa5d-2f2381c53495"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "data_dir = OUTPUT_PATH\n",
        "\n",
        "datagen_kwargs = dict(rescale=1./255, validation_split=.20)\n",
        "dataflow_kwargs = dict(target_size=IMAGE_SIZE, batch_size=BATCH_SIZE,\n",
        "                   interpolation=\"bilinear\")\n",
        "\n",
        "valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    **datagen_kwargs)\n",
        "valid_generator = valid_datagen.flow_from_directory(\n",
        "    data_dir, subset=\"validation\", shuffle=False, **dataflow_kwargs)\n",
        "\n",
        "do_data_augmentation = False\n",
        "\n",
        "if do_data_augmentation:\n",
        "  train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "      rotation_range=40,\n",
        "      horizontal_flip=True,\n",
        "      width_shift_range=0.2, height_shift_range=0.2,\n",
        "      shear_range=0.2, zoom_range=0.2,\n",
        "      **datagen_kwargs)\n",
        "else:\n",
        "  train_datagen = valid_datagen\n",
        "  train_generator = train_datagen.flow_from_directory(\n",
        "    data_dir, subset=\"training\", shuffle=True, **dataflow_kwargs)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 282 images belonging to 4 classes.\n",
            "Found 1139 images belonging to 4 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etCPHcWdmuq9",
        "outputId": "2b44add0-b269-4387-c555-2b65d1cfe8ff"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    # hub.KerasLayer(MODULE_HANDLE, dtype=tf.uint8),\n",
        "    # hub.KerasLayer(MODULE_HANDLE, trainable=True),\n",
        "    base_model,\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dropout(rate=0.2),\n",
        "    tf.keras.layers.Dense(4, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
        "])\n",
        "model.build((None,)+IMAGE_SIZE+(3,))\n",
        "model.summary()\n",
        "# opt = tfa.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-4)\n",
        "model.compile(loss='categorical_hinge', optimizer='adam', metrics=['accuracy'])\n",
        "# callbacks from tensor"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "mobilenetv2_1.00_224 (Functi (None, 7, 7, 1280)        2257984   \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 62720)             0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 62720)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4)                 250884    \n",
            "=================================================================\n",
            "Total params: 2,508,868\n",
            "Trainable params: 2,474,756\n",
            "Non-trainable params: 34,112\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ga12o179m1r7"
      },
      "source": [
        " do_fine_tuning = False"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6O6SVaZnBc1",
        "outputId": "b082b450-dee3-472d-8652-d4cef441f4f7"
      },
      "source": [
        "start = time.time()\n",
        "\n",
        "steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
        "validation_steps = valid_generator.samples // valid_generator.batch_size\n",
        "hist = model.fit(\n",
        "    train_generator,\n",
        "    epochs=5, steps_per_epoch=steps_per_epoch,\n",
        "    validation_data=valid_generator,\n",
        "    validation_steps=validation_steps).history\n",
        "\n",
        "end = time.time()\n",
        "print(\"Total train time: \",(end-start)/60,\" mins\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "10/35 [=======>......................] - ETA: 2:12 - loss: 1.1782 - accuracy: 0.4104"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2NscNJynDvI"
      },
      "source": [
        "loss = hist['loss']\n",
        "val_loss = hist['val_loss']\n",
        "acc = hist[\"accuracy\"]\n",
        "val_acc = hist[\"val_accuracy\"]\n",
        "epochs = range(len(val_acc))\n",
        "\n",
        "plt.figure()\n",
        "plt.title(\"Loss (training and validation)\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "\n",
        "plt.ylim([0,2])\n",
        "plt.plot(loss, 'r', label='Training loss')\n",
        "plt.plot(val_loss, 'b', label='Validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "plt.title(\"Accuracy (training and validation)\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "\n",
        "plt.ylim([0,1])\n",
        "plt.plot(acc, 'r', label='Training Accuracy')\n",
        "plt.plot(val_acc, 'b', label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}